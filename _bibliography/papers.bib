---
---

@inproceedings{liu2022crossa11y, 
author = {Liu, Xingyu "Bruce" and Wang, Ruolin and Li, Dingzeyu and Chen, Xiang "Anthony" and Pavel, Amy},
title = {CrossA11y: Identifying Video Accessibility Issues via Cross-Modal Grounding},
abstract = {Authors make their videos visually accessible by adding audio descriptions (AD), and auditorily accessible by adding closed captions (CC). However, creating AD and CC is challenging and tedious, especially for non-professional describers and captioners, due to the difficulty of identifying accessibility problems in videos. A video author will have to watch the video through and manually check for inaccessible information frame-by-frame, for both visual and auditory modalities. In this paper, we present CrossA11y, a system that helps authors efficiently detect and address visual and auditory accessibility issues in videos. Using cross-modal grounding analysis, CrossA11y automatically measures accessibility of visual and audio segments in a video by checking for modality asymmetries. CrossA11y then displays these segments and surfaces visual and audio accessibility issues in a unified interface, making it intuitive to locate, review, script AD/CC in-place, and preview the described and captioned video immediately. We demonstrate the effectiveness of CrossA11y through a lab study with 11 participants, comparing to existing baseline.},
year = {2022},
isbn = {9781450393201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3526113.3545703},
url={https://doi.org/10.1145/3526113.3545703},
booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {43},
numpages = {14},
keywords = {audio description, video, accessibility, closed caption},
location = {Bend, OR, USA},
series = {UIST '22},

abbr={UIST},
award={Best Paper Award},
bibtex_show={true},
pdf={liu2022crossa11y.pdf},
video={https://youtu.be/HDqjnHOZ7J8},
website={https://xybruceliu.github.io/CrossA11y/},
preview={crossa11y.gif},
selected={true},
}


@inproceedings{liu2021what,
author = {Liu, Xingyu and Carrington, Patrick and Chen, Xiang "Anthony" and Pavel, Amy},
title = {What Makes Videos Accessible to Blind and Visually Impaired People?},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445233},
doi = {10.1145/3411764.3445233},
abstract = {User-generated videos are an increasingly important source of information online, yet most online videos are inaccessible to blind and visually impaired (BVI) people. To find videos that are accessible, or understandable without additional description of the visual content, BVI people in our formative studies reported that they used a time-consuming trial-and-error approach: clicking on a video, watching a portion, leaving the video, and repeating the process. BVI people also reported video accessibility heuristics that characterize accessible and inaccessible videos. We instantiate 7 of the identified heuristics (2 audio-related, 2 video-related, and 3 audio-visual) as automated metrics to assess video accessibility. We collected a dataset of accessibility ratings of videos by BVI people and found that our automatic video accessibility metrics correlated with the accessibility ratings (Adjusted R2 = 0.642). We augmented a video search interface with our video accessibility metrics and predictions. BVI people using our augmented video search interface selected an accessible video more efficiently than when using the original search interface. By integrating video accessibility metrics, video hosting platforms could help people surface accessible videos and encourage content creators to author more accessible products, improving video accessibility for all.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {272},
numpages = {14},
keywords = {visual impairments, blind, online videos, accessibility},
location = {Yokohama, Japan},
series = {CHI '21},

abbr={CHI},
bibtex_show={true},
pdf={liu2021what.pdf},
video={https://youtu.be/n2enrJJZdTs},
website={https://dl.acm.org/doi/10.1145/3411764.3445233},
selected={true},
}


@inproceedings{gleason2019making,
author = {Gleason, Cole and Pavel, Amy and Liu, Xingyu and Carrington, Patrick and Chilton, Lydia B. and Bigham, Jeffrey P.},
title = {Making Memes Accessible},
year = {2019},
isbn = {9781450366762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308561.3353792},
doi = {10.1145/3308561.3353792},
abstract = {Images on social media platforms are inaccessible to people with vision impairments due to a lack of descriptions that can be read by screen readers. Providing accurate alternative text for all visual content on social media is not yet feasible, but certain subsets of images, such as internet memes, offer affordances for automatic or semi-automatic generation of alternative text. We present two methods for making memes accessible semi-automatically through (1) the generation of rich alternative text descriptions and (2) the creation of audio macro memes. Meme authors create alternative text templates or audio meme templates, and insert placeholders instead of the meme text. When a meme with the same image is encountered again, it is automatically recognized from a database of meme templates. Text is then extracted and either inserted into the alternative text template or rendered in the audio template using text-to-speech. In our evaluation of meme formats with 10 Twitter users with vision impairments, we found that most users preferred alternative text memes because the description of the visual content conveys the emotional tone of the character. As the preexisting templates can be automatically matched to memes using the same visual image, this combined approach can make a large subset of images on the web accessible, while preserving the emotion and tone inherent in the image memes.},
booktitle = {Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility},
pages = {367â€“376},
numpages = {10},
keywords = {blind, social media, audio, image description, alternative text, meme, low vision},
location = {Pittsburgh, PA, USA},
series = {ASSETS '19},

abbr={ASSETS},
bibtex_show={true},
pdf={gleason2019making.pdf},
blog={https://time.com/5759721/meme-accessibility-blind/},
website={https://dl.acm.org/doi/10.1145/3308561.3353792},
selected={true},
}